{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "pd.options.display.max_seq_items = 8000\n",
    "pd.options.display.max_rows = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./dataset/train.csv')\n",
    "df_test  = pd.read_csv('./dataset/test_9K3DBWQ.csv')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = pd.read_excel('./dataset/Data_Dictionary.xlsx')\n",
    "df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "#Check the new distribution\n",
    "sns.distplot(df_train['cc_cons'], color=\"b\");\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"cc_cons\")\n",
    "ax.set(title=\"cc_cons distribution\")\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % df_train['cc_cons'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train['cc_cons'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id , account_type , gender,age , region_code , loan_enq\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_dtypes = ['int64', 'float64']\n",
    "numeric = []\n",
    "for i in df_train.columns:\n",
    "    if df_train[i].dtype in numeric_dtypes:\n",
    "        if i in ['id', 'account_type','gender','age','region_code','loan_enq']:\n",
    "            pass\n",
    "        else:\n",
    "            numeric.append(i)\n",
    "# visualising some more outliers in the data values\n",
    "fig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\n",
    "plt.subplots_adjust(right=2)\n",
    "plt.subplots_adjust(top=2)\n",
    "sns.color_palette(\"husl\", 8)\n",
    "for i, feature in enumerate(list(df_train[numeric]), 1):\n",
    "    if(feature=='MiscVal'):\n",
    "        break\n",
    "    plt.subplot(len(list(numeric)), 3, i)\n",
    "    sns.scatterplot(x=feature, y='cc_cons', hue='cc_cons', palette='Blues', data=df_train)\n",
    "        \n",
    "    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n",
    "    plt.ylabel('cc_cons', size=15, labelpad=12.5)\n",
    "    \n",
    "    for j in range(2):\n",
    "        plt.tick_params(axis='x', labelsize=12)\n",
    "        plt.tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "    plt.legend(loc='best', prop={'size': 10})\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_train.corr()\n",
    "plt.subplots(figsize=(15,12))\n",
    "sns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_train['cc_cons'], df_train['cc_cons_apr']], axis=1)\n",
    "data.plot.scatter(x='cc_cons_apr', y='cc_cons', alpha=0.3, ylim=(0,200000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Ids from train and test, as they are unique for each row and hence not useful for the model\n",
    "train_ID = df_train['id']\n",
    "test_ID = df_test['id']\n",
    "df_train.drop('id', axis=1, inplace=True)\n",
    "df_test.drop('id', axis=1, inplace=True)\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train.copy()\n",
    "df_test_copy = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "#Check the new distribution \n",
    "sns.distplot(df_train['cc_cons'], color=\"b\");\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"cc_cons\")\n",
    "ax.set(title=\"cc_cons distribution\")\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outlier\n",
    "df_train.drop(df_train[(df_train['cc_cons']>250000)].index, inplace=True)\n",
    "df_train.drop(df_train[(df_train['cc_cons']<50)].index, inplace=True)\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"cc_cons\"] = np.log1p(df_train[\"cc_cons\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "#Check the new distribution \n",
    "sns.distplot(df_train['cc_cons'] , fit=norm, color=\"b\");\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df_train['cc_cons'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"cc_cons\")\n",
    "ax.set(title=\"cc_cons distribution\")\n",
    "sns.despine(trim=True, left=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and labels\n",
    "train_labels = df_train['cc_cons'].reset_index(drop=True)\n",
    "train_features = df_train.drop(['cc_cons'], axis=1)\n",
    "test_features = df_test\n",
    "\n",
    "# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\n",
    "all_features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
    "all_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_missing(df):\n",
    "    data = pd.DataFrame(df)\n",
    "    df_cols = list(pd.DataFrame(data))\n",
    "    dict_x = {}\n",
    "    for i in range(0, len(df_cols)):\n",
    "        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n",
    "    \n",
    "    return dict_x\n",
    "\n",
    "missing = percent_missing(all_features)\n",
    "df_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\n",
    "print('Percent of missing data')\n",
    "df_miss[0:42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "sns.set_style(\"white\")\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "sns.set_color_codes(palette='deep')\n",
    "missing = round(df_train.isnull().mean()*100,2)\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot.bar(color=\"b\")\n",
    "# Tweak the visual presentation\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Percent of missing values\")\n",
    "ax.set(xlabel=\"Features\")\n",
    "ax.set(title=\"Percent missing data by feature\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill 0 in 'personal_loan_active', 'vehicle_loan_active', 'personal_loan_closed',\n",
    "#        'vehicle_loan_closed',\n",
    "\n",
    "for i in all_features.columns:\n",
    "    if i in [ 'personal_loan_active', 'vehicle_loan_active', 'personal_loan_closed', 'vehicle_loan_closed']:\n",
    "        all_features[i].fillna(0, inplace=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan_enq fill 0, and change Y to 1\n",
    "all_features['loan_enq'].replace('Y',1, inplace=True)\n",
    "all_features['loan_enq'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features['card_lim'][all_features['card_lim']<10000]=np.nan\n",
    "all_features['card_lim'] = all_features.groupby('age')['card_lim'].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan 'investment_1', 'investment_2', 'investment_3', 'investment_4' to 0\n",
    "for i in all_features.columns:\n",
    "    if i in [ 'investment_1', 'investment_2', 'investment_3', 'investment_4']:\n",
    "        all_features[i].fillna(0, inplace=True)\n",
    "        \n",
    "all_features['total_investment']=all_features['investment_1']+all_features['investment_2']+all_features['investment_3']+all_features['investment_4']\n",
    "# all_features.drop(['investment_1','investment_2','investment_3','investment_4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dc_cons_apr', 'dc_count_apr\n",
    "# 'dc_cons_may', 'dc_count_may'\n",
    "all_features['dc_cons_apr'].fillna(0, inplace=True)\n",
    "all_features['dc_count_apr'].fillna(0, inplace=True) \n",
    "all_features['dc_cons_may'].fillna(0, inplace=True)\n",
    "all_features['dc_count_may'].fillna(0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.update(all_features[['dc_cons_jun', 'dc_count_jun']][(all_features['dc_cons_jun'].isnull()) & (all_features['dc_count_jun'].isnull())].fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('dc_cons_jun', 17.73),\n",
    "all_features['dc_cons_jun'] = all_features.groupby('age')['dc_cons_jun'].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ('debit_amount_may', 7.04),\n",
    "#  ('debit_count_may', 6.33),\n",
    "\n",
    "#  ('debit_amount_jun', 6.7),\n",
    "#   ('debit_count_jun', 4.66),\n",
    "   \n",
    "#  ('debit_amount_apr', 5.84),\n",
    "#  ('debit_count_apr', 5.43),\n",
    "all_features.update(all_features[['debit_amount_may', 'debit_count_may']][(all_features['debit_amount_may'].isnull()) & (all_features['debit_count_may'].isnull())].fillna(0))\n",
    "all_features.update(all_features[['debit_amount_jun', 'debit_count_jun']][(all_features['debit_amount_jun'].isnull()) & (all_features['debit_count_jun'].isnull())].fillna(0))\n",
    "all_features.update(all_features[['debit_amount_apr', 'debit_count_apr']][(all_features['debit_amount_apr'].isnull()) & (all_features['debit_count_apr'].isnull())].fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ('debit_amount_jun', 2.04),\n",
    "#  ('debit_amount_may', 0.71),\n",
    "#  ('debit_amount_apr', 0.41),\n",
    "\n",
    "all_features['debit_amount_jun'] = all_features.groupby('age')['debit_amount_jun'].transform(lambda x: x.fillna(x.median()))\n",
    "all_features['debit_amount_may'] = all_features.groupby('age')['debit_amount_may'].transform(lambda x: x.fillna(x.median()))\n",
    "all_features['debit_amount_apr'] = all_features.groupby('age')['debit_amount_apr'].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ('credit_amount_jun', 4.7),\n",
    "#  ('max_credit_amount_jun', 4.7),\n",
    "# ('credit_amount_may', 10.45),\n",
    "#  ('max_credit_amount_may', 10.45),\n",
    "#  ('credit_amount_apr', 10.17),\n",
    "#  ('max_credit_amount_apr', 10.17),\n",
    "\n",
    "all_features['credit_amount_may'].fillna(0, inplace=True)\n",
    "all_features['max_credit_amount_may'].fillna(0, inplace=True) \n",
    "all_features['credit_amount_apr'].fillna(0, inplace=True)\n",
    "all_features['max_credit_amount_apr'].fillna(0, inplace=True)\n",
    "all_features['credit_amount_jun'].fillna(0, inplace=True)\n",
    "all_features['max_credit_amount_jun'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# ('cc_count_may', 3.62),\n",
    "# ('cc_count_apr', 7.33),\n",
    "# ('cc_count_jun', 4.74),\n",
    "\n",
    "for col in ['cc_count_may','cc_count_apr','cc_count_jun']:\n",
    "    all_features[col] = all_features.groupby(['age','card_lim'])[col].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('cc_count_apr', 0.38),\n",
    "# ('cc_count_jun', 0.22),\n",
    "# ('cc_count_may', 0.21),\n",
    "\n",
    "for col in ['cc_count_may','cc_count_apr','cc_count_jun']:\n",
    "    all_features[col] = all_features.groupby(['age'])[col].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('credit_count_may', 6.33),\n",
    "# ('credit_count_apr', 5.43),\n",
    "# ('credit_count_jun', 4.66),\n",
    "\n",
    "for col in ['credit_count_may','credit_count_apr', 'credit_count_jun']:\n",
    "    all_features[col] = all_features.groupby(['age'])[col].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = percent_missing(all_features)\n",
    "df_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\n",
    "print('Percent of missing data')\n",
    "df_miss[0:42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.to_csv('./dataset/all_feature_210719.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.read_csv('./dataset/all_feature_210719.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the non-numeric predictors are stored as numbers; convert them into strings \n",
    "\n",
    "for col in ('region_code', 'age', 'personal_loan_active', 'vehicle_loan_active', 'personal_loan_closed', 'vehicle_loan_closed', 'loan_enq'):\n",
    "    all_features[col] = all_features[col].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all numeric features\n",
    "numeric_dtypes = ['int64', 'float64']\n",
    "\n",
    "numeric = []\n",
    "for i in all_features.columns:\n",
    "    if all_features[i].dtype in numeric_dtypes:\n",
    "        numeric.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots for all numeric features\n",
    "sns.set_style(\"white\")\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.set_xscale(\"log\")\n",
    "ax = sns.boxplot(data=all_features[numeric] , orient=\"h\", palette=\"Set1\")\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Feature names\")\n",
    "ax.set(xlabel=\"Numeric values\")\n",
    "ax.set(title=\"Numeric Distribution of Features\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find skewed numerical features\n",
    "skew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "\n",
    "high_skew = skew_features[skew_features >0.5]\n",
    "skew_index = high_skew.index\n",
    "\n",
    "print(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\n",
    "skewness = pd.DataFrame({'Skew' :high_skew})\n",
    "skew_features.head(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in skew_index:\n",
    "\n",
    "    all_features[i] = np.log1p(all_features[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.set_xscale(\"log\")\n",
    "ax = sns.boxplot(data=all_features[numeric] , orient=\"h\", palette=\"Set1\")\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Feature names\")\n",
    "ax.set(xlabel=\"Numeric values\")\n",
    "ax.set(title=\"Numeric Distribution of Features\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find skewed numerical features\n",
    "skew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "\n",
    "high_skew = skew_features[skew_features >0.5]\n",
    "skew_index = high_skew.index\n",
    "print(skew_index)\n",
    "\n",
    "print(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\n",
    "skewness = pd.DataFrame({'Skew' :high_skew})\n",
    "skew_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in skew_index:\n",
    "\n",
    "    all_features[i] = np.cbrt(all_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.set_xscale(\"log\")\n",
    "ax = sns.boxplot(data=all_features[numeric] , orient=\"h\", palette=\"Set1\")\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Feature names\")\n",
    "ax.set(xlabel=\"Numeric values\")\n",
    "ax.set(title=\"Numeric Distribution of Features\")\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find skewed numerical features\n",
    "skew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "\n",
    "high_skew = skew_features[skew_features >0.5]\n",
    "skew_index = high_skew.index\n",
    "print(skew_index)\n",
    "\n",
    "print(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\n",
    "skewness = pd.DataFrame({'Skew' :high_skew})\n",
    "skew_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.get_dummies(all_features).reset_index(drop=True)\n",
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duplicated column names\n",
    "all_features = all_features.loc[:,~all_features.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.to_csv('./dataset/all_features_final_210719.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features=pd.read_csv('./dataset/all_features_final_210719.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_features.iloc[:len(train_labels), :]\n",
    "X_test = all_features.iloc[len(train_labels):, :]\n",
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_x = MinMaxScaler()\n",
    "print(scaler_x.fit(all_features))\n",
    "all_features=scaler_x.transform(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.isfinite(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[np.isnan(X)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('./dataset/X_210719.csv', index=False)\n",
    "X_test.to_csv('./dataset/X_test_210719.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels=pd.DataFrame(train_labels)\n",
    "df_labels.to_csv('./dataset/df_labels_210719.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup cross validation and define error metrics¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross validation folds\n",
    "kf = KFold(n_splits=3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define error metrics\n",
    "def rmsle(y, y_pred):\n",
    "    return mean_squared_log_error(y, y_pred)*100\n",
    "\n",
    "def cv_rmse(model, X=X):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light Gradient Boosting Regressor\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                       num_leaves=6,\n",
    "                       learning_rate=0.01, \n",
    "                       n_estimators=7000,\n",
    "                       max_bin=200, \n",
    "                       bagging_fraction=0.8,\n",
    "                       bagging_freq=4, \n",
    "                       bagging_seed=8,\n",
    "                       feature_fraction=0.2,\n",
    "                       feature_fraction_seed=8,\n",
    "                       min_sum_hessian_in_leaf = 11,\n",
    "                       verbose=-1,\n",
    "                       random_state=42)\n",
    "\n",
    "# XGBoost Regressor\n",
    "xgboost = XGBRegressor(learning_rate=0.01,\n",
    "                       n_estimators=6000,\n",
    "                       max_depth=4,\n",
    "                       min_child_weight=0,\n",
    "                       gamma=0.6,\n",
    "                       subsample=0.7,\n",
    "                       colsample_bytree=0.7,\n",
    "                       objective='reg:squarederror',\n",
    "                       nthread=-1,\n",
    "                       scale_pos_weight=1,\n",
    "                       seed=27,\n",
    "                       reg_alpha=0.00006,\n",
    "                       random_state=42)\n",
    "\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=6000,\n",
    "                                learning_rate=0.01,\n",
    "                                max_depth=4,\n",
    "                                max_features='sqrt',\n",
    "                                min_samples_leaf=15,\n",
    "                                min_samples_split=10,\n",
    "                                loss='huber',\n",
    "                                random_state=42)  \n",
    "\n",
    "# Ridge Regressor\n",
    "ridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\n",
    "ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n",
    "\n",
    "# Support Vector Regressor\n",
    "svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=1200,\n",
    "                          max_depth=15,\n",
    "                          min_samples_split=5,\n",
    "                          min_samples_leaf=5,\n",
    "                          max_features=None,\n",
    "                          oob_score=True,\n",
    "                          random_state=42)\n",
    "\n",
    "# Stack up all the models above, optimized using gbr\n",
    "stack_gen_6 = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n",
    "                                meta_regressor=gbr,\n",
    "                                use_features_in_secondary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(lightgbm)\n",
    "print(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['lgb'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(xgboost)\n",
    "print(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['xgb'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(gbr)\n",
    "print(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['gbr'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(svr)\n",
    "print(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['svr'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(ridge)\n",
    "print(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['ridge'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(rf)\n",
    "print(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['rf'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Svr')\n",
    "svr_model_full_data = svr.fit(X, train_labels)\n",
    "\n",
    "# save the model to disk\n",
    "filename = './models/svr_210719.sav'\n",
    "pickle.dump(svr_model_full_data, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge')\n",
    "ridge_model_full_data = ridge.fit(X, train_labels)\n",
    "\n",
    "# save the model to disk\n",
    "filename = './models/ridge_210719.sav'\n",
    "pickle.dump(ridge_model_full_data, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RandomForest')\n",
    "rf_model_full_data = rf.fit(X, train_labels)\n",
    "\n",
    "# save the model to disk\n",
    "filename = './models/randomforest_210719.sav'\n",
    "pickle.dump(rf_model_full_data, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('stack_gen')\n",
    "stack_gen_6model = stack_gen_6.fit(np.array(X), np.array(train_labels))\n",
    "\n",
    "# save the model to disk\n",
    "filename = './models/stack_gen_6model_210719.sav'\n",
    "pickle.dump(stack_gen_6model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lightgbm_210719')\n",
    "lgb_model_full_data = lightgbm.fit(X, train_labels)\n",
    "\n",
    "# save the model to disk\n",
    "filename = './models/lightgbm_210719.sav'\n",
    "pickle.dump(lgb_model_full_data, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('xgboost_210719')\n",
    "xgb_model_full_data = xgboost.fit(X, train_labels)\n",
    "\n",
    "# save the model to disk\n",
    "filename = './models/xgboost_210719.sav'\n",
    "pickle.dump(xgb_model_full_data, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('gradientboosting_210719')\n",
    "gbr_model_full_data = gbr.fit(X, train_labels)\n",
    "\n",
    "# save the model to disk\n",
    "filename = './models/gradientboosting_210719.sav'\n",
    "pickle.dump(gbr_model_full_data, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[np.isnan(X_test)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blended7_predictions(X):\n",
    "    return ((0.05 * ridge_model_full_data.predict(X)) + \\\n",
    "            (0.1 * svr_model_full_data.predict(X)) + \\\n",
    "            (0.15 * gbr_model_full_data.predict(X)) + \\\n",
    "            (0.1 * xgb_model_full_data.predict(X)) + \\\n",
    "            (0.15 * lgb_model_full_data.predict(X)) + \\\n",
    "            (0.1 * rf_model_full_data.predict(X)) + \\\n",
    "            (0.35 * stack_gen_6model.predict(np.array(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = blended7_predictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sample_submission dataframe\n",
    "submission = pd.read_csv(\"./dataset/sample_submission_iwBpW0t.csv\")\n",
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append predictions from blended models\n",
    "submission.iloc[:,1] = np.floor(np.expm1(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix outleir predictions\n",
    "q1 = submission['cc_cons'].quantile(0.00035)\n",
    "q2 = submission['cc_cons'].quantile(0.99)\n",
    "submission['cc_cons'] = submission['cc_cons'].apply(lambda x: x if x > q1 else x*0.80)\n",
    "submission['cc_cons'] = submission['cc_cons'].apply(lambda x: x if x < q2 else x*1.08)\n",
    "submission.to_csv(\"./dataset/submission_blended71_210719.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale predictions\n",
    "submission['cc_cons'] *= 0.9845\n",
    "submission.to_csv(\"./dataset/submission_blended72_210719.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
